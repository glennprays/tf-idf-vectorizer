{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Extracting\n",
        "Extract document files into dataset"
      ],
      "metadata": {
        "id": "UY9nevXldlGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEoiAVO0YZQ3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "DOCS_DIR = '/content/drive/MyDrive/Classroom/Sistem Temu Balik Informasi IF-910 (AB) Ganjil 2023 2024/Tubes/combined'\n",
        "# list of docs name\n",
        "DOCS_FILE_NAME = os.listdir(DOCS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_json(filename, data):\n",
        "  with open(filename, \"w\") as file:\n",
        "        json.dump(data, file)\n",
        "\n",
        "def load_json(filename):\n",
        "  with open(filename, \"r\") as file:\n",
        "    return json.load(file)"
      ],
      "metadata": {
        "id": "PLlOMJuhyIBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the docs list into json\n",
        "dump_json(\"docs_list.json\", DOCS_FILE_NAME)"
      ],
      "metadata": {
        "id": "Gehd5rn7yitQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# storing documents into dataset list\n",
        "dataset = []\n",
        "\n",
        "for doc in DOCS_FILE_NAME:\n",
        "  file_path = os.path.join(DOCS_DIR, doc)\n",
        "  file = open(file_path, 'r')\n",
        "  with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
        "    text = file.read().replace('\\n', ' ').strip()\n",
        "  text = nltk.word_tokenize(text)\n",
        "  file.close()\n",
        "\n",
        "  dataset.append(text)"
      ],
      "metadata": {
        "id": "8yH7I_dTaDzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5164e1ca-d1b7-4352-d103-2b42c22d643e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dump_json(\"raw_dataset.json\", dataset)"
      ],
      "metadata": {
        "id": "UoQT-VkpzU-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprosessing"
      ],
      "metadata": {
        "id": "WwqinK-fdwpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to lowercase"
      ],
      "metadata": {
        "id": "7D0OPJkjgZm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "9paY5sWjhCVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_lower_case(doc):\n",
        "  return np.char.lower(np.array(doc, dtype=np.str_))"
      ],
      "metadata": {
        "id": "5_ZB2VgmdesZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop words"
      ],
      "metadata": {
        "id": "5MrjndyGgjh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "JLHwZbgWgpIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50660d02-e16d-48f4-e9e6-749f1ea987f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def stop_word_filtering(doc):\n",
        "  return [word for word in doc if word not in stop_words]"
      ],
      "metadata": {
        "id": "8dkJYgC3i8IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punctuation Remover"
      ],
      "metadata": {
        "id": "rqiiql0wknbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "translation_table = str.maketrans(\"\", \"\", string.punctuation)"
      ],
      "metadata": {
        "id": "gQXJ0BFilI_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(doc):\n",
        "  return [word.translate(translation_table) for word in doc if word.translate(translation_table) != '']"
      ],
      "metadata": {
        "id": "ysBInV9Zk5hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Word Remover"
      ],
      "metadata": {
        "id": "03QIL_4BmQz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_printable_from_words(doc):\n",
        "    def remove_non_printable(word):\n",
        "        return ''.join(char for char in word if char in string.printable)\n",
        "\n",
        "    return [remove_non_printable(word) for word in doc]"
      ],
      "metadata": {
        "id": "yyUro50EmSy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apostrophe Remover"
      ],
      "metadata": {
        "id": "ABIwZpm_nM_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_apostrophe(doc):\n",
        "  return [word.replace(\"'\", \"\") for word in doc]"
      ],
      "metadata": {
        "id": "8nzgWvQDnPii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Char Remover"
      ],
      "metadata": {
        "id": "pPo3O53AoCfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_single_char(doc):\n",
        "  return [word for word in doc if len(word) > 1]"
      ],
      "metadata": {
        "id": "WIxxrdhhn4Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "0NuLTqkvovWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "jzDmSnwMpAmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_stemming(doc):\n",
        "  return [porter_stemmer.stem(word) for word in doc]"
      ],
      "metadata": {
        "id": "_b4WdjNIox0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatisation"
      ],
      "metadata": {
        "id": "GN2RIG4LpQrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "fcofARKFph8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff71ee2-c032-4875-d664-d98f32217b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def word_lemmatization(doc):\n",
        "  return [lemmatizer.lemmatize(word, pos=get_wordnet_pos(word)) for word in doc]"
      ],
      "metadata": {
        "id": "m3DDe4EDpTIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Numbers to word"
      ],
      "metadata": {
        "id": "8eEIde8XrSA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "from num2words import num2words"
      ],
      "metadata": {
        "id": "isgYtbfQrWgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f19457-7a04-4c08-8570-cb7457e8e7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m122.9/143.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=020177af2a43973efb43ce60f5d16f2fb7ac8b661fec6230bae01e19a87cd55a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_string_number(s):\n",
        "    try:\n",
        "        int_value = int(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def number_to_word(num):\n",
        "  words = num2words(num)\n",
        "  token = nltk.word_tokenize(words)\n",
        "  doc = convert_lower_case(token)\n",
        "  return doc\n",
        "\n",
        "\n",
        "def convert_number(doc):\n",
        "    result = []\n",
        "    for word in doc:\n",
        "        if is_string_number(word):\n",
        "            result.extend(number_to_word(word))\n",
        "        else:\n",
        "            result.append(word)\n",
        "    return result"
      ],
      "metadata": {
        "id": "yh_PD2-rsRHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Preprocessing"
      ],
      "metadata": {
        "id": "2uyoVAuywvri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(doc):\n",
        "  doc = convert_number(doc)\n",
        "  doc = convert_lower_case(doc)\n",
        "  doc = stop_word_filtering(doc)\n",
        "  doc = remove_non_printable_from_words(doc)\n",
        "  doc = remove_punctuation(doc)\n",
        "  doc = remove_apostrophe(doc)\n",
        "  doc = remove_single_char(doc)\n",
        "  doc = word_lemmatization(doc)\n",
        "  doc = word_stemming(doc)\n",
        "\n",
        "  return doc\n"
      ],
      "metadata": {
        "id": "ligORftvwzo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the dataset"
      ],
      "metadata": {
        "id": "rBmkgsoq1Pzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "NN7KaN6J2Oa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_json(\"raw_dataset.json\")"
      ],
      "metadata": {
        "id": "cgh55iWz1PSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the dataset"
      ],
      "metadata": {
        "id": "O_mvQx4g2XT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_docs = []\n",
        "\n",
        "for doc in dataset:\n",
        "  preprocessed_docs.append(preprocess(doc))"
      ],
      "metadata": {
        "id": "r5fJ5sAb2jLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_json(\"docs_preprocessed.json\", preprocessed_docs)"
      ],
      "metadata": {
        "id": "wGh-xQOT3J21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "pFfZXrfCUppg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Term Frequencies (TF)"
      ],
      "metadata": {
        "id": "IDjqJJFfUy_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tf(docs):\n",
        "    tf = []\n",
        "    for doc in docs:\n",
        "        doc_tf = {}\n",
        "        for word in doc:\n",
        "            doc_tf[word] = doc_tf.get(word, 0) + 1\n",
        "        tf.append(doc_tf)\n",
        "    return tf"
      ],
      "metadata": {
        "id": "3bZw0_4vUyM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "-q6ReUPVVFHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "cGwWs-zGVloQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_idf(docs):\n",
        "    idf = {}\n",
        "    total_documents = len(docs)\n",
        "\n",
        "    for doc in docs:\n",
        "        unique_words = set(doc)\n",
        "        for word in unique_words:\n",
        "            idf[word] = idf.get(word, 0) + 1\n",
        "\n",
        "    for word, count in idf.items():\n",
        "        idf[word] = math.log10(total_documents / count)\n",
        "\n",
        "    return idf\n"
      ],
      "metadata": {
        "id": "n5vJnayoVLEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate TF-IDF"
      ],
      "metadata": {
        "id": "goPgazJUVv6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tfidf(docs, tf, idf):\n",
        "    tfidf = []\n",
        "    for i, doc in enumerate(docs):\n",
        "        doc_tfidf = {}\n",
        "        for word in doc:\n",
        "            doc_tfidf[word] = tf[i][word] * idf.get(word, 0)\n",
        "        tfidf.append(doc_tfidf)\n",
        "    return tfidf"
      ],
      "metadata": {
        "id": "B2-odEXbV1sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consine Similarity"
      ],
      "metadata": {
        "id": "ANQen9azWB_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(query_tfidf, doc_tfidf):\n",
        "    dot_product = sum(query_tfidf.get(word, 0) * doc_tfidf.get(word, 0) for word in set(query_tfidf) & set(doc_tfidf))\n",
        "    query_norm = math.sqrt(sum(value**2 for value in query_tfidf.values()))\n",
        "    document_norm = math.sqrt(sum(value**2 for value in doc_tfidf.values()))\n",
        "\n",
        "    if query_norm == 0 or document_norm == 0:\n",
        "        return 0\n",
        "\n",
        "    similarity = dot_product / (query_norm * document_norm)\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "zDEVwpOwWE71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Matching Documents"
      ],
      "metadata": {
        "id": "r-jU6gPpWNs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_matching_documents(query_tfidf, documents_tfidf):\n",
        "    similarities = [cosine_similarity(query_tfidf, doc_tfidf) for doc_tfidf in documents_tfidf]\n",
        "\n",
        "    matching_documents = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
        "    return matching_documents"
      ],
      "metadata": {
        "id": "SKZjT3jkWTKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Documents using Query"
      ],
      "metadata": {
        "id": "I2HfYkccWehX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load preprocessed documents"
      ],
      "metadata": {
        "id": "wV_r_EvjYdYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = load_json(\"docs_preprocessed.json\")"
      ],
      "metadata": {
        "id": "aHGsTkxeYkqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Documents TF-IDF"
      ],
      "metadata": {
        "id": "Sm9bHL5VYLiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = calculate_tf(docs)\n",
        "idf = calculate_idf(docs)\n",
        "docs_tfidf = calculate_tfidf(docs, tf, idf)"
      ],
      "metadata": {
        "id": "1-_A_1N9Wt6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dump_json(\"docs_tf.json\", tf)\n",
        "dump_json(\"docs_idf.json\", idf)\n",
        "dump_json(\"docs_tfidf.json\", docs_tfidf)"
      ],
      "metadata": {
        "id": "iF5W9v6Hz3oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Preprocessing"
      ],
      "metadata": {
        "id": "iHZnUj2OZNOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"In 2006, the UK decides on approving the EU constitution, seen by some as crucial for EU efficiency and by others as a step towards federalism.\"\n",
        "query = \"indonesia independent day\"\n",
        "preprocessed_query = preprocess(nltk.word_tokenize(query))"
      ],
      "metadata": {
        "id": "G2S5Tl6GZRgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query TF-IDF"
      ],
      "metadata": {
        "id": "t9qwgF-kbro3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_tfidf = calculate_tfidf([preprocessed_query], calculate_tf([preprocessed_query]), idf)[0]"
      ],
      "metadata": {
        "id": "FNr55ZxYbuOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_matching_documents(query_tfidf, docs_tfidf)"
      ],
      "metadata": {
        "id": "wxvKG_Yz6yaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[tupel for tupel in result if tupel[1] > 0.09]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM8fDWa7IjBm",
        "outputId": "d6649b4c-d145-440e-a328-456e180fbbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3859, 0.29804171501263027),\n",
              " (11, 0.26472209044781947),\n",
              " (615, 0.166472471186825),\n",
              " (717, 0.10436083338482859)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DOCS_FILE_NAME[result[0][0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h2jxAkgmIIok",
        "outputId": "9d3b4425-36fb-4432-f858-30ff7fea0c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'indo58'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}